<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Artifact 2 — Narrative</title>
  <link rel="stylesheet" href="../../styles.css"> <!-- optional shared styles -->
  <style>
    :root{
      --bg:#0b1220; --panel:#0e1630; --ink:#eef2ff; --muted:#98a3c7;
      --blue:#3b82f6; --gold:#fbbf24; --stroke:rgba(255,255,255,.09);
      --radius:18px; --shadow:0 10px 30px rgba(0,0,0,.35);
    }
    html,body{height:100%}
    body{
      margin:0; color:var(--ink);
      font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;
      background:
        radial-gradient(900px 600px at 0% -10%, rgba(59,130,246,.18), transparent),
        radial-gradient(800px 500px at 110% 10%, rgba(251,191,36,.15), transparent),
        var(--bg);
    }
    /* Constrain and center existing top-level elements without adding wrappers */
    body > *{
      width:min(1000px,92vw);
      margin-left:auto; margin-right:auto;
    }
    h1{margin:44px 0 10px; font-size:clamp(24px,3.2vw,36px)}
    /* Narrative block as a bento tile; preserve line breaks/tabs */
    body > p:first-of-type{
      margin:0 0 18px; padding:18px;
      background:var(--panel); border:1px solid var(--stroke);
      border-radius:var(--radius); box-shadow:var(--shadow);
      white-space:pre-wrap; line-height:1.55; color:var(--ink);
    }
    /* Back link paragraph spacing */
    body > p:last-child{margin:22px 0 40px}
    /* Back link styled as a subtle button */
    a[href*="index.html"]{
      display:inline-flex; align-items:center; gap:8px;
      padding:10px 14px; border-radius:12px; text-decoration:none;
      color:var(--ink);
      background:linear-gradient(180deg, rgba(255,255,255,.06), rgba(255,255,255,.02));
      border:1px solid var(--stroke);
      box-shadow:var(--shadow); font-weight:600;
    }
    a[href*="index.html"]:hover{filter:brightness(1.05)}
  </style>
</head>
<body>
  <h1>Artifact 2 — Narrative</h1>
    <p>The original artifact, titled “Design Defense Project 2,” was created approximately six months ago as part of my coursework in reinforcement learning. It documents my approach to training a Deep Q-Learning agent, represented as a pirate, to solve a maze and locate hidden treasure. The agent was developed using a neural network and trained through trial and error, adjusting its behavior using rewards and penalties. This artifact served as a foundation to demonstrate my understanding of reinforcement learning principles and how algorithms can be applied to decision making tasks.
	For my ePortfolio, I chose to enhance this artifact to better demonstrate my competency in algorithms and data structures. The original pirate agent was a guided learning experience, so I decided to create a new, self developed project to show my ability to independently apply these concepts. I built a Tic Tac Toe agent entirely from scratch using Q-Learning. This new version showcases my ability to define the game environment, implement state management, design a reward structure, and iteratively improve performance through self play. Unlike the original artifact, which involved a neural network framework, this version uses a Q-table, allowing me to directly manipulate and observe how different parameters affect learning. The agent learns to win or draw consistently against random opponents, demonstrating that it is not only functional but effectively trained.
	This enhancement meets the learning outcomes I outlined in Module One, particularly my goal to demonstrate hands on skills in building intelligent agents using algorithmic thinking and reinforcement strategies. It expands on the original artifact by stripping away external frameworks and requiring me to manage all components of the agent directly, from environment setup to learning logic. This updated version strengthens the coverage of my algorithmic competency by highlighting my ability to independently construct and train machine learning models.
	Through this enhancement, I deepened my understanding of how reinforcement learning agents use exploration, exploitation, and reward signals to build effective decision policies. One key challenge I faced was balancing the simplicity of the implementation with the need for meaningful performance. Since the Tic Tac Toe environment is small, overtraining or poor reward structure could lead to erratic or overly conservative agents. Tuning the learning rate, exploration decay, and reward penalties was a valuable exercise in algorithmic thinking. Creating this from scratch allowed me to better understand how data structures like dictionaries and arrays support algorithm behavior, and how these choices influence the efficiency and learning quality of the agent. This process strengthened both my confidence and skill in applying core CS principles to real world problems.

</p>

  <p><a href="../../index.html">← Back to ePortfolio</a></p>
</body>
</html>